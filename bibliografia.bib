@inproceedings{Wilde2008,
abstract = {Much of the Web{\&}{\#}x2019;s success rests with its role in enabling information reuse and integration across various boundaries. Hyperlinked Web resources represent a rich information tapestry of content and context, instrumental in effective knowledge sharing and further knowledge development. However, the Web{\&}{\#}x2019;s simple linking model has become increasingly inadequate for effective content discovery and reuse. At the same time, rigorous but heavyweight solutions such as the Semantic Web have yet to garner critical mass in adoption. This paper analyzes the relative strengths and shortcomings of existing linked data approaches. It proposes a novel, lightweight architecture for the modeling, aggregation, retrieval, management, and sharing of contextual information for Web resources, based on established standards and designed to encourage more efficient and robust information reuse on the Web.},
author = {Wilde, Erik and {Yiming Liu}},
booktitle = {2008 IEEE International Conference on Information Reuse and Integration},
doi = {10.1109/IRI.2008.4583029},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilde, Liu - 2008 - Lightweight linked data.pdf:pdf},
isbn = {978-1-4244-2659-1},
pages = {196--201},
publisher = {IEEE},
title = {{Lightweight linked data}},
url = {http://ieeexplore.ieee.org/document/4583029/},
year = {2008}
}

@misc{Cyganiak2014,
author = {Cyganiak, Richard and Jentzsch, Anja},
title = {{The Linking Open Data cloud diagram}},
url = {http://lod-cloud.net/},
urldate = {2016-12-02},
year = {2014}
}


@incollection{Auer2007,
abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information can be made available on the Web for humans and machines. We describe some emerging applications from the DBpedia community and show how website operators can reduce costs by facilitating royalty-free DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data sources.},
author = {Auer, S{\"{o}}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-76298-0_52},
editor = {Aberer, Karl and and Choi, Key-Sun and and Noy, Natasha and and Allemang, Dean and and Lee, Kyung-Il and and Nixon, Lyndon and and Golbeck, Jennifer and and Mika, Peter and and Maynard, Diana and and Mizoguchi, Riichiro and and Schreiber, Guus and and Cudr{\{}$\backslash$'e{\}}-Mauroux, Philippe},
file = {:home/fpierin/Dropbox/artigos/dbpedia.pdf:pdf},
isbn = {3540762973},
issn = {03029743},
pages = {722--735},
publisher = {Springer Berlin Heidelberg},
title = {{DBpedia: A nucleus for a Web of open data}},
url = {http://link.springer.com/10.1007/978-3-540-76298-0{\_}52},
volume = {4825 LNCS},
year = {2007}
}

@article{Mika2015,
abstract = {How do we keep the Web searchable as it expands and evolves?},
author = {Mika, Peter},
doi = {10.1109/MIC.2015.81},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mika - 2015 - On Schema.org and Why It Matters for the Web.pdf:pdf},
isbn = {1089-7801},
issn = {1089-7801},
journal = {Internet Computing, IEEE},
keywords = {Data Representation,Html,Information Retrieval,Internet,Internet/Web Technologies,Linked Data,Protocols,Schema.Org,Search Engines,Semantic Technology,Semantics,Vocabulary,Web Search},
number = {4},
pages = {52--55},
title = {{On Schema.org and Why It Matters for the Web}},
volume = {19},
year = {2015}
}

@incollection{Tort2014,
author = {Tort, Albert and Oliv{\'{e}}, Antoni},
booktitle = {Conceptual Modeling: 33rd International Conference, ER 2014, Atlanta, GA, USA, October 27-29, 2014. Proceedings},
doi = {10.1007/978-3-319-12206-9_3},
editor = {Yu, Eric and and Dobbie, Gillian and and Jarke, Matthias and and Purao, Sandeep},
file = {:home/fpierin/Dropbox/artigos/chp{\%}3A10.1007{\%}2F978-3-319-12206-9{\_}3.pdf:pdf},
isbn = {9783319122052},
issn = {16113349},
keywords = {conceptual modeling,microdata,ontologies,org,schema},
pages = {28--42},
publisher = {Springer International Publishing},
title = {{A Computer-Guided Approach to Website Schema.org Design}},
url = {http://link.springer.com/10.1007/978-3-319-12206-9{\_}3},
year = {2014}
}

@article{Bienvenu2013,
abstract = {Ontology-based data access is concerned with querying incomplete data sources in the presence of domain-specific knowledge provided by an ontology. A central notion in this setting is that of an ontology-mediated query, which is a database query coupled with an ontology. In this paper, we study several classes of ontology-mediated queries, where the database queries are given as some form of conjunctive query and the ontologies are formulated in description logics or other relevant fragments of first-order logic, such as the guarded fragment and the unary-negation fragment. The contributions of the paper are three-fold. First, we characterize the expressive power of ontology-mediated queries in terms of fragments of disjunctive datalog. Second, we establish intimate connections between ontology-mediated queries and constraint satisfaction problems (CSPs) and their logical generalization, MMSNP formulas. Third, we exploit these connections to obtain new results regarding (i) first-order rewritability and datalog-rewritability of ontology-mediated queries, (ii) P/NP dichotomies for ontology-mediated queries, and (iii) the query containment problem for ontology-mediated queries.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.6479v2},
author = {Bienvenu, Meghyn and ten Cate, Balder and Lutz, Carsten and Wolter, Frank},
doi = {10.1145/2463664.2465223},
eprint = {arXiv:1301.6479v2},
file = {:home/fpierin/Dropbox/artigos/1301.6479v2.pdf:pdf},
isbn = {9781450320665},
issn = {15574644},
journal = {Proceedings of the 32nd symposium on Principles of database systems - PODS '13},
keywords = {ontology-based data access,query answering,query rewriting},
number = {4},
pages = {213},
title = {{Ontology-based data access}},
url = {http://dl.acm.org/citation.cfm?doid=2463664.2465223},
volume = {39},
year = {2013}
}

@article{Civili2013,
author = {Civili, Cristina and Ruzzi, Marco and Santarelli, Valerio and Savo, Domenico Fabio and Console, Marco and {De Giacomo}, Giuseppe and Lembo, Domenico and Lenzerini, Maurizio and Lepore, Lorenzo and Mancini, Riccardo and Poggi, Antonella and Rosati, Riccardo},
doi = {10.14778/2536274.2536304},
file = {:home/fpierin/Dropbox/artigos/p803-poggi.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
month = {aug},
number = {12},
pages = {1314--1317},
title = {{Mastro Studio: Managing Ontology-based Data Access Applications}},
url = {http://dl.acm.org/citation.cfm?doid=2536274.2536304},
volume = {6},
year = {2013}
}

@incollection{Lembo2014,
address = {Cham},
author = {Lembo, Domenico and Mora, Jose and Rosati, Riccardo and Savo, Domenico Fabio and Thorstensen, Evgenij},
booktitle = {Web Reasoning and Rule Systems: 8th International Conference, RR 2014, Athens, Greece, September 15-17, 2014. Proceedings},
doi = {10.1007/978-3-319-11113-1_8},
editor = {Kontchakov, Roman and Mugnier, Marie-Laure},
file = {:home/fpierin/Dropbox/artigos/LRRST-RR-14.pdf:pdf},
isbn = {978-3-319-11113-1},
issn = {16113349},
pages = {108--123},
publisher = {Springer International Publishing},
title = {{Towards Mapping Analysis in Ontology-Based Data Access}},
url = {http://link.springer.com/10.1007/978-3-319-11113-1{\_}8},
year = {2014}
}


@incollection{Kharlamov2013,
abstract = {The recently started EU FP7-funded project Optique will develop an end-to-end OBDA system providing scalable end-user access to industrial Big Data stores. This paper presents an initial architectural specification for the Optique system along with the individual system components. {\textcopyright} Springer-Verlag 2013.},
author = {Kharlamov, Evgeny and Jim{\'{e}}nez-Ruiz, Ernesto and Zheleznyakov, Dmitriy and Bilidas, Dimitris and Giese, Martin and Haase, Peter and Horrocks, Ian and Kllapi, Herald and Koubarakis, Manolis and {\"{O}}z{\c{c}}ep, {\"{O}}zg{\"{u}}r and Rodr{\'{i}}guez-Muro, Mariano and Rosati, Riccardo and Schmidt, Michael and Schlatte, Rudolf and Soylu, Ahmet and Waaler, Arild},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-41242-4_11},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kharlamov et al. - 2013 - Optique Towards OBDA systems for industry.pdf:pdf},
isbn = {9783642412417},
issn = {03029743},
keywords = {Big data,OBDA,OWL 2,Ontologies,System architecture},
pages = {125--140},
publisher = {Springer Berlin Heidelberg},
title = {{Optique: Towards OBDA systems for industry}},
url = {http://link.springer.com/10.1007/978-3-642-41242-4{\_}11},
volume = {7955 LNCS},
year = {2013}
}

@inproceedings{Ahmed2008,
abstract = {Traditional legacy HTML based web sites/ page can be thought of as web services because the dynamic web pages can take user input argument via web forms and response to user query. The ability of agents and services to automatically locate and interact with unknown partners is a goal for Web based Data Integration system. This ldquoserendipitous interoperabilityrdquo is hindered by the lack of an explicit means of describing what web pages are able to do and in order to do it what input it takes and what output it produces, that is what is their capabilities [1]. The tremendous success of the WWW is countervailed by the efforts needed to search and find relevant information. For tabular structures embedded in HTML documents, typical keyword or link-analysis based search fails. The next phase envisioned for the WWW is automatic ad-hoc interaction between intelligent agents, web services, databases and semantic web enabled applications. A large amount of information available on the Web is formatted in HTML tables, which are mainly presentation oriented and are not suited for database applications. As a result, how to capture information in HTML tables semantically and integrate relevant information is a challenge. We are envisioning another layer of web abstraction where user can query intra web document table like structure. Our prototype application is based on WebFusion and an ad hoc query language BioFlow [2], [3], [4], [5], [6] a software agent that can simulate a person interacting with web search forms and extracting information from the resulting pages by means of an API. We need to develop a framework which is able to query search web forms and the web page tables in a SQL way. In this context we also report a Java based implementation for integrating Flybase and AlignACE site.},
author = {Ahmed, Emdad},
booktitle = {Proceedings of 11th International Conference on Computer and Information Technology, ICCIT 2008},
doi = {10.1109/ICCITECHN.2008.4802991},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahmed - 2008 - Resource capability discovery and description management system for bioinformatics data and service integration - An expe.pdf:pdf},
isbn = {9781424421367},
keywords = {HTML forms,Hidden Web,Intelligent Wrapper,Ontology generation,Semantic Web,Table modeling,Table structure,Web Automation,Web Data Integration,Web Information Extraction,Web mining,extraction ontology},
month = {dec},
pages = {56--61},
publisher = {IEEE},
title = {{Resource capability discovery and description management system for bioinformatics data and service integration - An experiment with gene regulatory networks}},
url = {http://ieeexplore.ieee.org/document/4802991/},
year = {2008}
}

@article{Heath2008,
abstract = {Web2.0 has enabled contributions to the Web on an unprecedented scale, through simple interfaces that provide engaging interactions. This wealth of data has spawned countless mashups that integrate heterogenous information, but using techniques that will not scale beyond a handful of sources. In contrast, the Semantic Web provides the key to large-scale data integration, yet still lacks approachable interfaces allowing contributions from non-specialists. In this paper we present Revyu, a reviewing and rating site in the Web2.0 mould that is built on Semantic Web infrastructure and both publishes and consumes linked RDF data. This combination of approaches affords ease of interaction for regular users and ease of integration with external data sources. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Heath, Tom and Motta, Enrico},
doi = {10.1016/j.websem.2007.11.009},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heath, Motta - 2008 - Ease of interaction plus ease of integration Combining Web2.0 and the Semantic Web in a reviewing site.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semantics},
keywords = {Data integration,Reviews,Semantic Web,User interaction,Web2.0},
number = {1},
pages = {76--83},
title = {{Ease of interaction plus ease of integration: Combining Web2.0 and the Semantic Web in a reviewing site}},
volume = {6},
year = {2008}
}

@inproceedings{Sui2009,
abstract = {To overcome the disadvantages of the existing recommender system such as intelligence, adaptive, flexibility ability and the limitations of accuracy and efficiency for the results, the framework model of Integration with Semantic Web and Agent Personalized Recommendation System in e-commerce (SWAPRS) is designed based on Semantic Web and agent and web mining technology. The function module of SWAPRS is constructed to be an autonomy intelligent agent by means of agent technology in this architecture. The work flow of recommender system and function of module are discussed in this paper.},
author = {Sui, Xin and Wang, Suozhu and Li, Zhaowei},
booktitle = {2009 13th International Conference on Computer Supported Cooperative Work in Design},
doi = {10.1109/CSCWD.2009.4968064},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sui, Wang, Li - Unknown - Research on the Model of Integration with Semantic Web and Agent Personalized Recommendation System.pdf:pdf},
isbn = {978-1-4244-3534-0},
keywords = {agent,difficult for the systems,individual needs of users,personalization,recommendation system,semantic web,so it is very,to meet the},
pages = {233--237},
publisher = {IEEE},
title = {{Research on the model of Integration with Semantic Web and Agent Personalized Recommendation System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4968064},
year = {2009}
}

@inproceedings{Wei2013,
author = {Wei, Wu and Shi, Shengsheng and Liu, Yulong and Wang, Haitao and Yuan, Chunfeng and Huang, Yihua},
booktitle = {Proceedings - 2013 10th Web Information System and Application Conference, WISA 2013},
doi = {10.1109/WISA.2013.21},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wei et al. - 2013 - Extraction Rule Language for Web Information Extraction and Integration.pdf:pdf},
isbn = {978-1-4799-3219-1},
keywords = {Data record,Extraction model,Extraction rule language,Web information extraction},
month = {nov},
pages = {65--70},
publisher = {IEEE},
title = {{Extraction rule language for web information extraction and integration}},
url = {http://ieeexplore.ieee.org/document/6778612/},
year = {2013}
}

@inproceedings{Vettor2014,
abstract = {Nowadays, the Web offers huge amounts of data sources for the benefit of the community. However, there is a lack of practical approach for converting and linking multi-origin data sources into one coherent smart data set. In this paper, we define a service-oriented architecture to attach explicit semantics to data, to solve heterogeneity issues, and to remove data inconsistencies in order to convert raw documents to quality Linked Data. We motivate the need for a service oriented architecture for smart data with a live scenario based on the Audience Labs company information system. We show how our service-oriented architecture adapts to the company needs and facilitates semantic annotation, data integration and exploitation of the resulting smart data. {\textcopyright} 2014 IEEE.},
author = {Vettor, Pierre De and Mrissa, Michael and Benslimane, Djamal and Berbar, Salim},
booktitle = {2014 IEEE 8th International Symposium on Service Oriented System Engineering},
doi = {10.1109/SOSE.2014.30},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vettor et al. - 2014 - A Service Oriented Architecture for Linked Data Integration.pdf:pdf},
isbn = {978-1-4799-3616-8},
keywords = {data integration,data semantics,service oriented architecture,smart data},
month = {apr},
pages = {198--203},
publisher = {IEEE},
title = {{A Service Oriented Architecture for Linked Data Integration}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84903640868{\&}partnerID=tZOtx3y1},
year = {2014}
}

@inproceedings{Sleiman2011,
abstract = {Companies comprise a variety of software applications to carry out their business activities. A recurrent challenge is how to make them interoperate with each other which is usually handcrafted, which is a tedious task that increases integration costs. Enterprise Service Buses range amongst the most popular solution to reduce these costs, and they allow to implement integration solutions by means of one or more layers between software applications and business processes. In this paper, we present a framework for information extraction that allow to wrap information from different web sources and to generate linked data. Furthermore, we survey a number of approaches in the bibliography to build Enterprise Service Buses in the context of semantic-web technologies, which comprise RDF, RDFS, OWL, and SPARQL languages. Finally, we conclude that, thanks to linked data, we may integrate software applications with other applications that generate and/or consume these linked data.},
author = {Sleiman, Hassan a. and Rivero, Carlos R. and Corchuelo, Rafael},
booktitle = {2011 7th International Conference on Next Generation Web Services Practices},
doi = {10.1109/NWeSP.2011.6088199},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sleiman, Rivero, Corchuelo - 2011 - On a proposal to integrate web sources using semantic-web technologies.pdf:pdf},
isbn = {978-1-4577-1127-5},
keywords = {Semantic web services and linked data,Semantic web services architectures},
month = {oct},
pages = {326--331},
publisher = {IEEE},
title = {{On a proposal to integrate web sources using semantic-web technologies}},
url = {http://ieeexplore.ieee.org/document/6088199/},
year = {2011}
}


@article{Jain2011,
author = {Jain, Ankita and Khan, Ilyas and Verma, Bhupendra},
doi = {10.5120/1962-2625},
file = {:home/fpierin/Dropbox/artigos/26425d0b0aff8b37d8a05b947f9137b64d41.pdf:pdf},
issn = {09758887},
journal = {International Journal of Computer Applications},
keywords = {association mining,ontology,owl,semantic web mining},
month = {feb},
number = {7},
pages = {14--18},
title = {{Secure and Intelligent Decision making in Semantic web mining}},
url = {http://www.ijcaonline.org/volume15/number7/pxc3872625.pdf},
volume = {15},
year = {2011}
}

@article{Quboa2013,
abstract = {The integration of the two fast-developing scientific research areas Semantic Web and Web Mining is known as Semantic Web Mining. The huge increase in the amount of Semantic Web data became a perfect target for many researchers to apply Data Mining techniques on it. This paper gives a detailed state-of-the-art survey of on-going research in this new area. It shows the positive effects of Semantic Web Mining, the obstacles faced by researchers and propose number of approaches to deal with the very complex and heterogeneous information and knowledge which are produced by the technologies of Semantic Web.},
author = {Quboa, Qudamah K and Saraee, Mohamad},
doi = {10.4236/iim.2013.51002},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quboa, Saraee - 2013 - A State-of-the-Art Survey on Semantic Web Mining.pdf:pdf},
issn = {2160-5912},
journal = {Intelligent Information Management},
keywords = {data mining,semantic web,semantic web mining,web mining},
number = {January},
pages = {10--17},
title = {{A State-of-the-Art Survey on Semantic Web Mining}},
url = {http://dx.doi.org/10.4236/iim.2013.51002 http://www.scirp.org/journal/iim http://www.scirp.org/journal/PaperDownload.aspx?DOI=10.4236/iim.2013.51002},
volume = {05},
year = {2013}
}

@article{Stumme2006,
abstract = {Semantic Web Mining aims at combining the two fast-developing research areas Semantic Web and Web Mining. This survey analyzes the convergence of trends from both areas: More and more researchers are working on improving the results of Web Mining by exploiting semantic structures in the Web, and they make use of Web Mining techniques for building the Semantic Web. Last but not least, these techniques can be used for mining the Semantic Web itself. The Semantic Web is the second-generation WWW, enriched by machine-processable information which supports the user in his tasks. Given the enormous size even of today's Web, it is impossible to manually enrich all of these resources. Therefore, automated schemes for learning the relevant information are increasingly being used. Web Mining aims at discovering insights about the meaning of Web resources and their usage. Given the primarily syntactical nature of the data being mined, the discovery of meaning is impossible based on these data only. Therefore, formalizations of the semantics of Web sites and navigation behavior are becoming more and more common. Furthermore, mining the Semantic Web itself is another upcoming application. We argue that the two areas Web Mining and Semantic Web need each other to fulfill their goals, but that the full potential of this convergence is not yet realized. This paper gives an overview of where the two areas meet today, and sketches ways of how a closer integration could be profitable.},
author = {Stumme, G and Hotho, A and Berendt, B},
doi = {10.1016/j.websem.2006.02.001},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stumme, Hotho, Berendt - 2006 - Semantic Web MiningState of the art and future directions(2).pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
keywords = {artificial intelligence,knowledge discovery,knowledge engineering,ontologies,semantic web,web mining,world wide web},
month = {jun},
pages = {124--143},
title = {{Semantic Web Mining: State of the art and future directions}},
volume = {4},
year = {2006}
}


@inproceedings{Zhang2011,
abstract = {With the prompt increase of information on the WWW, user Web mining has gradually become more and more important in data mining. People always hope to gain some efficient knowledge patterns through searching, integrating, mining and analyzing on the Web. These useful knowledge patterns can help us to build an efficient Web site that can serve people better. The research on text mining and usage mining on the Web are introduced. We give an applicable example of usage mining.},
author = {Zhang, Haiyang},
booktitle = {2011 International Conference on Management and Service Science},
doi = {10.1109/ICMSS.2011.5999196},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haiyang - Unknown - The Research of Web Mining in E-commerce.pdf:pdf},
isbn = {978-1-4244-6579-8},
keywords = {-Web miningt,data mining,style,web content mining},
month = {aug},
pages = {1--4},
publisher = {IEEE},
title = {{The Research of Web Mining in E-Commerce}},
url = {http://ieeexplore.ieee.org/document/5999196/},
volume = {3},
year = {2011}
}

@article{Liu2002,
abstract = { With the prompt increase of information on the WWW, user Web mining has gradually become more and more important in data mining. People always hope to gain some efficient knowledge patterns through searching, integrating, mining and analyzing on the Web. These useful knowledge patterns can help us to build an efficient Web site that can serve people better. The research on text mining and usage mining on the Web are introduced. We give an applicable example of usage mining.},
author = {Liu, Lizhen and Chen, Junjie and Song, Hantao},
doi = {10.1109/WCICA.2002.1021507},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haiyang - Unknown - The Research of Web Mining in E-commerce.pdf:pdf},
isbn = {0780372689},
keywords = {-Web miningt,data mining,style,web content mining},
pages = {2333--2337},
title = {{The Research of Web Mining}},
year = {2002}
}

@article{Kabir2014,
abstract = {Semantic web offers a smarter web service which synchronizes and arranges all the data over web in a disciplined manner. In data mining over web, the accuracy of selecting necessary data according to user demand and pick them for output is considered as a major challenging task over the years. This paper proposes an approach to mapping data over the web 3.0 through ontology and access the required data via an intelligent agent. The agent provides all the searched data related to user query from which user can find desired information. When the user does not have sufficient search parameter, knowledge can be perceived from the information provided by the agent. The derivation of such unknown knowledge from the existing can be achieved by semantic web mining. We present an intelligent agent-based web mining model where users' query is being searched by following existing traditional way, e.g. by Google. The intelligent agent checks the searched data and derives only those are the semantically related to users search parameter. A work-in-progress case study of University Faculty Information presented to examine the effectiveness of the proposed model.},
author = {Kabir, Sumaiya and Ripon, Shamim and Rahman, Mamunur and Rahman, Tanjim},
doi = {10.1016/j.ieri.2014.08.018},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kabir et al. - 2014 - Knowledge-based Data Mining Using Semantic Web.pdf:pdf},
isbn = {2212-6678},
issn = {22126678},
journal = {IERI Procedia},
keywords = {Intelligent Agent;,Semantic Web,Web Mining},
pages = {113--119},
publisher = {Elsevier B.V.},
title = {{Knowledge-based Data Mining Using Semantic Web}},
url = {http://www.sciencedirect.com/science/article/pii/S2212667814000379},
volume = {7},
year = {2014}
}

@misc{Clark1999,
author = {Clark, James and DeRose, Steve},
booktitle = {W3C Recommendation,},
keywords = {path language,xpath},
number = {November},
pages = {1--37},
title = {{XML path language (XPath) version 1.0}},
url = {https://www.w3.org/TR/1999/REC-xpath-19991116/},
year = {1999}
}

@incollection{Perez2006,
abstract = {SPARQL is the W3C candidate recommendation query lan- guage for RDF. In this paper we address systematically the formal study of SPARQL, concentrating in its graph pattern facility. We consider for this study simple RDF graphs without special semantics for literals and a simplified version of filters which encompasses all the main issues. We provide a compositional semantics, prove there are normal forms, prove complexity bounds, among others that the evaluation of SPARQL patterns is PSPACE-complete, compare our semantics to an alternative operational semantics, give simple and natural conditions when both se- mantics coincide and discuss optimization procedures.},
archivePrefix = {arXiv},
arxivId = {cs/0605124},
author = {P{\'{e}}rez, Jorge and Arenas, Marcelo and Gutierrez, Claudio},
booktitle = {The Semantic Web - ISWC 2006},
doi = {10.1007/11926078_3},
eprint = {0605124},
file = {:home/fpierin/Dropbox/artigos/sparql.pdf:pdf},
isbn = {978-3-540-49055-5},
issn = {0302-9743},
pages = {30--43},
publisher = {"Springer Berlin Heidelberg"},
primaryClass = {cs},
title = {{Semantics and Complexity of SPARQL}},
url = {http://www.springerlink.com/index/10.1007/11926078_3},
volume = {4273},
year = {2006}
}

@incollection{Quilitz2008,
abstract = {Integrated access to multiple distributed and autonomous RDF data sources is a key challenge for many semantic web applications. As a reaction to this challenge, SPARQL, the W3C Recommendation for an RDF query language, supports querying of multiple RDF graphs. However, the current standard does not provide transparent query federation, which makes query formulation hard and lengthy. Furthermore, current implementations of SPARQL load all RDF graphs mentioned in a query to the local machine. This usually incurs a large overhead in network traffic, and sometimes is simply impossible for technical or legal reasons. To overcome these problems we present DARQ, an engine for federated SPARQL queries. DARQ provides transparent query access to multiple SPARQL services, i.e., it gives the user the impression to query one single RDF graph despite the real data being distributed on the web. A service description language enables the query engine to decompose a query into sub-queries, each of which can be answered by an individual service. DARQ also uses query rewriting and cost-based query optimization to speed-up query execution. Experiments show that these optimizations significantly improve query performance even when only a very limited amount of statistical information is available. DARQ is available under GPL License at http://darq.sf.net/ .},
address = {Berlin, Heidelberg},
author = {Quilitz, Bastian and Leser, Ulf},
booktitle = {The Semantic Web: Research and Applications},
doi = {10.1007/978-3-540-68234-9_39},
file = {:home/fpierin/Dropbox/artigos/DARQ-FINAL.pdf:pdf},
isbn = {978-3-540-68233-2},
issn = {03029743},
pages = {524--538},
publisher = {Springer Berlin Heidelberg},
title = {{Querying Distributed RDF Data Sources with SPARQL}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-68234-9{\_}39 http://link.springer.com/10.1007/978-3-540-68234-9{\_}39},
volume = {5021 LNCS},
year = {2008}
}

@article{Calvanese2016,
abstract = {In this paper we present Ontop, an open-source Ontology Based Data Access (OBDA) system that allows for querying relational data sources through a conceptual representation of the domain of interest, provided in terms of an ontology, to which the data sources are mapped. Key features of Ontop are its solid theoretical foundations, a virtual approach to OBDA that avoids materializing triples and that is implemented through query rewriting techniques, extensive optimizations exploiting all elements of the OBDA architecture, its compliance to all relevant W3C recommendations (including SPARQL queries, R2RML mappings, and OWL 2 QL and RDFS ontologies), and its support for all major relational databases.},
author = {Calvanese, D and Cogrel, B and Komla-Ebri, S},
doi = {10.3233/SW-160217},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calvanese et al. - Unknown - Ontop Answering SPARQL Queries over Relational Databases.pdf:pdf},
issn = {22104968},
journal = {Semantic Web},
keywords = {databases,obda,ontologies,ontop,owl,r2rml,rdf,sparql},
number = {0},
title = {{Ontop: Answering SPARQL queries over relational databases}},
url = {http://content.iospress.com/articles/semantic-web/sw217},
volume = {0},
year = {2016}
}


@article{Wang,
abstract = {—Integrating data from multiple heterogeneous sources entail dealing with different data models, schemas and query languages. The burgeoning Semantic Web has provided several new methods for data integration. This paper focuses on integration of relational database and XML data. To solve the problem we propose an ontology-based approach. A semantic integration infrastructure for heterogeneous data sources is presented. In this infrastruc-ture, ontology is used as the mediated schema for the repre-sentation of the data source semantics. To model different source schemas, we propose a describing method based on RDF graph patterns. The semantic mappings between source schema and RDF ontology are described declarative-ly using SPARQL queries. The semantic of query rewriting is further discussed and a query rewriting algorithm is pre-sented.},
author = {Wang, Jinpeng and Lu, Jianjiang and Zhang, Yafei and Miao, Zhuang and Zhou, Bo},
doi = {10.4304/jsw.4.8.843-850},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - Integrating Heterogeneous Data Source Using Ontology.pdf:pdf},
issn = {1796217X},
journal = {Journal of Software},
keywords = {Data integration,Heterogeneous database,Ontology,RDF,SPARQL},
number = {8},
pages = {843--850},
title = {{Integrating heterogeneous data source using ontology}},
volume = {4},
year = {2009}
}

@inproceedings{VanDeursen2008,
abstract = {The role of metadata is gaining importance due to today's growth of multimedia content. Currently, XML is the standard for data interchange. However, as XML Schemas do not express semantics but rather the document structure, there is a lack of semantic interoperability regarding current (XML-based) metadata standards. By using semantic Web technologies, ontologies can be created to describe the semantics of a particular metadata format. A problem is that the existing XML data (compliant with a particul.ar metadata format) cannot be used by an ontology, implying the need for a conversion of XML data to RDF instances. In this paper, a generic approach is proposed for the transformation of XML data into RDF instances in an ontology-dependent way. By means of a mapping document, the link is described between an XML Schema (describing the structure of particular XML data) and an OWL ontology. Our approach is illustrated by applying it to the DIG35 specification, which is an XML-based metadata standard for the description of digital images.},
author = {Deursen, Davy Van and Poppe, Chris and Martens, G{\"{a}}etan and Mannens, Erik and de Walle, Rik Van},
booktitle = {2008 International Conference on Automated Solutions for Cross Media Content and Multi-Channel Distribution},
doi = {10.1109/AXMEDIS.2008.17},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Deursen et al. - 2008 - XML to RDF conversion A generic approach.pdf:pdf},
isbn = {978-0-7695-3406-0},
month = {nov},
pages = {138--144},
publisher = {IEEE},
title = {{XML to RDF Conversion: A Generic Approach}},
url = {http://ieeexplore.ieee.org/document/4688061/},
year = {2008}
}

@incollection{Gali2004,
abstract = {The semantic web envisions a World Wide Web in which data is described with rich semantics and applications can pose complex queries. Ontologies, a cornerstone of the semantic web, have gained wide popularity as a model of information in a given domain that can be used for many purposes, including enterprise integration, database design, information retrieval and information interchange on the World Wide Web. Much of the current focus on ontologies has been on the development of languages such as DAML+OIL and OWL that enable the creation of ontologies and provide extensive semantics for Web data, and on answering intensional queries, that is, queries about the structure of an ontology. However, it is almost certain that the many of the semantic web queries will be extensional and to flourish, the semantic web will need to accommodate the huge amounts of existing data that is described by the ontologies and the applications that operate on them. Given the established record of relational databases to store and query large amounts of data, in this paper we present a set of techniques to provide a lossless mapping of an OWL ontology to a relational schema and the corresponding instances to data. We present preliminary experiments that compare the efficiency of the mapping techniques in terms of query performance.},
author = {Gali, Anuradha and Chen, Cindy X. and Claypool, Kajal T. and Uceda-Sosa, Rosario},
booktitle = {Conceptual Modeling for Advanced Application Domains},
doi = {10.1007/978-3-540-30466-1_26},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gali, Chen - 2004 - From ontology to relational databases.pdf:pdf},
isbn = {978-3-540-23722-8},
issn = {03029743},
pages = {278--289},
publisher = {Springer, Berlin, Heidelberg},
title = {{From Ontology to Relational Databases}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-30466-1{\_}26 http://link.springer.com/10.1007/978-3-540-30466-1{\_}26},
year = {2004}
}


@article{Gruber1993,
abstract = {To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms. 1.},
author = {Gruber, Thomas R.},
doi = {10.1.1.101.7493},
file = {:home/fpierin/Dropbox/biblioteca/KSL-92-17.pdf:pdf},
isbn = {1042-8143},
issn = {10428143},
journal = {Knowledge Acquisition},
number = {2},
pages = {199--220},
pmid = {21685580},
title = {{A translation approach to portable ontology specifications}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.7493},
volume = {5},
year = {1993}
}

@article{Noy2001,
abstract = {Ontologies have become core components of many large applications yet the training material has not kept pace with the growing interest. This paper addresses the issues of why one would build an ontology and presents a methodology for creating ontologies based on declarative knowledge representation systems. It leverages the two authors experiences building and maintaining ontologies in a number of ontology environments including Protege-2000, Ontolingua, and Chimaera. It presents the methodology by example utilizing a tutorial wines knowledge base example. While it is aimed at users of frame-based systems, it can be useful for building ontologies in any object-centered system.},
archivePrefix = {arXiv},
arxivId = {1304.1186},
author = {Noy, Natalya F. and McGuinness, Deborah L.},
doi = {10.1016/j.artmed.2004.01.014},
eprint = {1304.1186},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Noy, McGuinness - 2001 - Ontology development 101 A guide to creating your first ontology.pdf:pdf},
isbn = {9783540237983},
issn = {09333657},
journal = {Stanford Knowledge Systems Laboratory},
month = {sep},
pages = {25},
pmid = {3342},
title = {{Ontology Development 101: A Guide to Creating Your First Ontology}},
year = {2001}
}


@article{Hellmann2009,
abstract = {In this paper we present Triplify {\{} a simplistic but e ective$\backslash$napproach to publish Linked Data from relational databases.$\backslash$nTriplify is based on mapping HTTP-URI requests onto relational$\backslash$ndatabase queries. Triplify transforms the resulting$\backslash$nrelations into RDF statements and publishes the data$\backslash$non the Web in various RDF serializations, in particular as$\backslash$nLinked Data. The rationale for developing Triplify is that$\backslash$nthe largest part of information on the Web is already stored$\backslash$nin structured form, often as data contained in relational$\backslash$ndatabases, but usually published by Web applications only$\backslash$nas HTML mixing structure, layout and content. In order$\backslash$nto reveal the pure structured information behind the current$\backslash$nWeb, we have implemented Triplify as a light-weight$\backslash$nsoftware component, which can be easily integrated into$\backslash$nand deployed by the numerous, widely installed Web applications.$\backslash$nOur approach includes a method for publishing$\backslash$nupdate logs to enable incremental crawling of linked data$\backslash$nsources. Triplify is complemented by a library of con gurations$\backslash$nfor common relational schemata and a REST-enabled$\backslash$ndata source registry. Triplify con gurations containing mappings$\backslash$nare provided for many popular Web applications, including$\backslash$nosCommerce, WordPress, Drupal, Gallery, and phpBB.$\backslash$nWe will show that despite its light-weight architecture$\backslash$nTriplify is usable to publish very large datasets, such as$\backslash$n160GB of geo data from the OpenStreetMap project.}},
author = {Hellmann, Sebastian and Hellmann, Sebastian and Auer, S{\"{o}}ren and Auer, S{\"{o}}ren and Dietzold, Sebastian and Dietzold, Sebastian and Lehmann, Jens and Lehmann, Jens and Aumueller, David and Aumueller, David},
doi = {10.1145/1526709.1526793},
isbn = {9781605584874},
journal = {Proceedings of the 18th International Conference on World Wide Web, Madrid},
keywords = {data web,databases,geo data,linked data,rdf,semantic web,sql,web application},
pages = {621--630},
title = {{Triplify – Light-Weight Linked Data Publication from Relational Databases}},
url = {http://www2009.org/proceedings/pdf/p621.pdf},
year = {2009}
}

@article{Vieira,
author = {Vieira, Andr\'{e} Accioly},
file = {:home/fpierin/Dropbox/biblioteca/014.pdf:pdf},
journal = {Language},
pages = {127--132},
title = {{OntoExtract : Uma Ferramenta para Extra\c{c}\~{a}o de Ontologias a Partir de Bancos de Dados Relacionais}},
year = {2002}
}

@article{Cunha2007,
author = {Cunha, Adriano and Albrecht, Felipe and Fernandes, Ricardo Queiroz De Araujo},
file = {:home/fpierin/Dropbox/biblioteca/544fab900cf278a1f19d8229.pdf:pdf},
pages = {1--27},
title = {{Inferencia sobre Ontologias: o reencontro com PROLOG}},
year = {2007}
}

@article{May,
abstract = {The Semantic Web aims at providing Web data sources on a semantic level. On the other hand, most of the Web data itself is not suitably prepared (e.g., by annotations). In this paper, we describe a semantic layer that integrates existing data sources with the Semantic Web by combining semantic modeling with links that associate the semantic notions with actual data on the Web. The semantic level consists of specialized service providers - which can be seen as agents - for each application domain. Each agent contains ontological knowledge represented in XML where the links to the actual data are embedded as XPath expressions, similar to XLink. The agent uses its knowledge with an internal reasoning mechanism to combine the links for translating a Semantic Web query into a Web query that is then evaluated against the individual sources.},
annote = {O artigo fala sobre um mecanismo capaz de aplicar uma camada sem{\^{a}}ntica sobre os dados xml e alguns casos html.},
author = {May, Wolfgang},
doi = {10.1109/DEXA.2002.1045882},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/May - 2002 - Linking the Semantic Web with existing sources.pdf:pdf},
isbn = {0769516688},
issn = {15294188},
journal = {Proceedings - International Workshop on Database and Expert Systems Applications, DEXA},
keywords = {Bridges,Data models,Joining processes,Microstrip,Ontologies,Semantic Web,Unified modeling language,XML},
pages = {93--97},
title = {{Linking the Semantic Web with existing sources}},
volume = {2002-January},
year = {2002}
}

@article{Bojars2008,
abstract = {Large volumes of content (bookmarks, reviews, videos, etc.) are currently being created on the "Social Web", i.e. on Web 2.0 community sites, and this content is being annotated and commented upon. The ability to view an individual's entire contribution to the Social Web would be an interesting and valuable service, particularly important as social networks are often being formed through created content and things that people have in common ("object-centred sociality"). SIOC is a Semantic Web research project that aims to describe online communities on the Social Web. This paper describes how SIOC and the Semantic Web can enable linking and reuse scenarios of data from Web 2.0 community sites, and introduces a SIOC Types module to further specify the type of content items and act as a "glue" between user posts and the content items created and annotated by users. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Bojars, U. and Breslin, J. G. and Finn, a. and Decker, S.},
doi = {10.1016/j.websem.2007.11.010},
file = {:home/fpierin/Dropbox/biblioteca/1-s2.0-S157082680700056X-main.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semantics},
keywords = {RDF,SIOC,Semantic Web,Social software,Web 2.0},
number = {1},
pages = {21--28},
title = {{Using the Semantic Web for linking and reusing data across Web 2.0 communities}},
volume = {6},
year = {2008}
}

@article{Battle2008,
abstract = {Semantic Web technologies must integrate with Web 2.0 services for both to leverage each others strengths. We argue that the REST-based design methodologies [R.T. Fielding, R.N. Taylor, Principled design of the modern web architecture, ACM Trans. Internet Technol. (TOIT) 2 (2) (2002) 115-150] of the web present the ideal mechanism through which to align the publication of semantic data with the existing web architecture. We present the design and implementation of two solutions that combine REST-based design and RDF [D. Beckett (Ed.), RDF/XML Syntax Specification (Revised), W3C Recommendation, February 10, 2004] data access: one solution for integrating existing web services and one server-side solution for creating RDF REST services. Both of these solutions enable SPARQL [E. Prud'hommeaux, A. Seaborne (Eds.), SPARQL Query Language for RDF, W3C Working Draft, March 26, 2007] to be a unifying data access layer for aligning the Semantic Web and Web 2.0. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Battle, Robert and Benson, Edward},
doi = {10.1016/j.websem.2007.11.002},
file = {:home/fpierin/Dropbox/biblioteca/1-s2.0-S1570826807000510-main.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Web Semantics},
keywords = {Representational State Transfer,Semantic Web,Web 2.0,Web Services},
number = {1},
pages = {61--69},
title = {{Bridging the semantic Web and Web 2.0 with Representational State Transfer (REST)}},
volume = {6},
year = {2008}
}

@article{AndreasHess,
abstract = {Web 2.0 and Semantic Web are regarded as two comple- mentary paradigms that will probably converge in the future. However, whereas the Semantic Web is an established eld of research, there has been little analysis devoted to Web 2.0 applications. For this reason it re- mains unclear how the advantages of both paradigms could be merged. In this paper we make three contributions in this direction. First, we discuss why merging Web 2.0 and the Semantic Web is bene cial and propose ve approaches. Second, we show that (semi-) automated tag- ging of content improves the quality of annotations. Third, we present an automatic approach for improving the tag quality by using duplicate detection techniques. We verify our approach on a large-scale data set from the social search service Lycos IQ.},
author = {{He\ss, Andreas}, Christian Maa\ss and Francis Dieric},
file = {:home/fpierin/Dropbox/biblioteca/cisweb2008\_submission\_6.pdf:pdf},
title = {{From Web 2.0 to Semantic Web: A Semi-Automated Approach}},
url = {http://www.ibiblio.org/hhalpin/homepage/notes/cisweb2008\_submission\_6.pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {20--34},
volume = {351},
year = {2008}
}

@article{Fayyad1996,
abstract = {Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Copyright © 1996, American Association for Artificial Intelligence. All rights reserved.},
author = {Fayyad, Usama and Piatetsky-Shapiro, G and Smyth, Padhraic},
doi = {10.1145/240455.240463},
file = {:home/fpierin/Dropbox/biblioteca/fayyad.pdf:pdf},
isbn = {0-262-56097-6},
issn = {0738-4602},
journal = {AI magazine},
pages = {37--54},
title = {{From data mining to knowledge discovery in databases}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/1230},
year = {1996}
}

@misc{RDFWorkingGroup2014,
abstract = {RDF is a standard model for data interchange on the Web. RDF has features that facilitate data merging even if the underlying schemas differ, and it specifically supports the evolution of schemas over time without requiring all the data consumers to be changed. RDF extends the linking structure of the Web to use URIs to name the relationship between things as well as the two ends of the link (this is usually referred to as a “triple”). Using this simple model, it allows structured and semi-structured data to be mixed, exposed, and shared across different applications. This linking structure forms a directed, labeled graph, where the edges represent the named link between two resources, represented by the graph nodes. This graph view is the easiest possible mental model for RDF and is often used in easy-to-understand visual explanations.},
author = {{RDF Working Group}},
keywords = {RDF},
mendeley-tags = {RDF},
title = {{RDF - Semantic Web Standards}},
url = {http://www.w3.org/RDF/},
urldate = {2015-02-05},
year = {2014}
}

@misc{W3C_SPARQL,
abstract = {RDF is a directed, labeled graph data format for representing information in the Web. This specification defines the syntax and semantics of the SPARQL query language for RDF. SPARQL can be used to express queries across diverse data sources, whether the data is stored natively as RDF or viewed as RDF via middleware. SPARQL contains capabilities for querying required and optional graph patterns along with their conjunctions and disjunctions. SPARQL also supports extensible value testing and constraining queries by source RDF graph. The results of SPARQL queries can be results sets or RDF graphs.},
author = {W3C},
booktitle = {W3C Recommendation 15 January 2008},
keywords = {SPARQL},
title = {{SPARQL Query Language for RDF}},
url = {http://www.w3.org/TR/rdf-sparql-query/},
urldate = {2015-05-02},
year = {2013}
}

@misc{W3C_XML,
  author = {W3C},
  booktitle = {W3C Recommendation 26 November 2008},
  keywords = {XML},
  title = {Extensible Markup Language (XML) 1.0 (Fifth Edition)},
  year = 2008,
  url = {https://www.w3.org/TR/2008/REC-xml-20081126/},
  urldate = {2008-08-26},
}

@misc{W3C_RDF,
  author = {W3C},
  booktitle = {Resource Description Framework (RDF)},
  keywords = {RDF},
  title = {RDF},
  year = 2014,
  url = {https://www.w3.org/RDF/},
  urldate = {2014-02-25},
}

@incollection{Berendt2004,
abstract = {The purpose of Web mining is to develop methods and systems for discovering$\backslash$nmodels of objects and processes on the World Wide Web and for web-based$\backslash$nsystems that show adaptive performance. Web Mining integrates three$\backslash$nparent areas: Data Mining (we use this term here also for the closely$\backslash$nrelated areas of Machine Learning and Knowledge Discovery), Internet$\backslash$ntechnology and World Wide Web, and for the more recent has made an$\backslash$nenormous amount of information electronically accessible. The use$\backslash$nof email, news and markup languages like HTML allow users to publish$\backslash$nand read documents at a world-wide scale and to communicate via chat$\backslash$nconnections, including information in the form of images and voice$\backslash$nrecords. The HTTP protocol that enables access to documents over$\backslash$nthe network via Web browsers created an immense improvement in communication$\backslash$nand access to information. For some years these possibilities were$\backslash$nused mostly in the scientific world but recent years have seen an$\backslash$nimmense growth in popularity, supported by the wide availability$\backslash$nof computers and broadband communication. The use of the internet$\backslash$nfor other tasks than finding information and direct communication$\backslash$nis increasing, as can be seen from the interest in e-activities such$\backslash$nas e-commerce, e-learning, e-government, e-science.},
address = {Berlin, Heidelberg},
author = {Berendt, Bettina and Hotho, Andreas and Mladenic, Dunja and van Someren, Maarten and Spiliopoulou, Myra and Stumme, Gerd},
booktitle = {Web Mining: From Web to Semantic Web},
doi = {10.1007/978-3-540-30123-3_1},
editor = {Berendt, Bettina and Hotho, Andreas and Mladeni{\v{c}}, Dunja and van Someren, Maarten and Spiliopoulou, Myra and Stumme, Gerd},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berendt, Hotho - 2004 - A roadmap for web mining From web to semantic web.pdf:pdf},
isbn = {3540416714},
issn = {16113349},
keywords = {information retrieval {\&} textual information access,learning,statistics {\&} optimisation},
pages = {1--22},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{A Roadmap for Web Mining: From Web to Semantic Web}},
url = {http://eprints.pascal-network.org/archive/00000841/{\%}5Cnhttp://www.springerlink.com/index/6c87nfe4t1qhc0h6.pdf http://link.springer.com/10.1007/b100615 http://link.springer.com/10.1007/978-3-540-30123-3{\_}1},
volume = {3209},
year = {2004}
}

@article{Califf2004,
abstract = {Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present an algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains.},
author = {Califf, Mary Elaine and Mooney, Raymond J},
doi = {10.1162/153244304322972685},
file = {:home/fpierin/Dropbox/biblioteca/rapier-jmlr-03.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {information extraction,natural language processing,relational learning},
pages = {177--210},
title = {{Bottom-Up Relational Learning of Pattern Matching Rules for Information Extraction}},
url = {http://www.crossref.org/jmlr\_DOI.html},
volume = {4},
year = {2004}
}

@article{Freitag2000,
abstract = {Recent work in machine learning for information extraction has focused on two distinct sub-problems: the conventional problem of filling template slots from natural language text, and the problem of wrapper induction, learning simple ex- traction procedures (wrappers) for highly structured text such asWeb pages produced by CGI scripts. For suitably reg- ular domains, existing wrapper induction algorithms can effi- ciently learnwrappers that are simple and highly accurate, but the regularity bias of these algorithms makes them unsuitable for most conventional information extraction tasks. Boost- ing is a technique for improving the performance of a simple machine learning algorithm by repeatedly applying it to the training set with different example weightings. We describe an algorithm that learns simple, low-coverage wrapper-like extraction patterns, which we then apply to conventional in- formation extraction problems using boosting. The result is BWI, a trainable information extraction system with a strong precision bias and F1 performance better than state-of-the-art techniques in many domains.},
author = {Freitag, Dayne and Kushmerick, Nicholas},
doi = {10.2307/3977872},
file = {:home/fpierin/Dropbox/biblioteca/AAAI00-088.pdf:pdf},
isbn = {0-262-51112-6},
journal = {Proceedings Of The National Conference On Artificial Intelligence},
number = {15},
pages = {577--583},
title = {{Boosted Wrapper Induction}},
url = {http://www.aaai.org/Papers/AAAI/2000/AAAI00-088.pdf},
volume = {145},
year = {2000}
}

@article{Hess2004,
abstract = {The semantic Web Services vision requires that each service be annotated with semantic metadata. Manually creating such metadata is tedious and error-prone, and many software engineers, accustomed to tools that automatically generate WSDL, might not want to invest the additional e\#ort. We therefore propose ASSAM, a tool that assists a user in creating semantic metadata for Web Services. ASSAM is intended for service consumers who want to integrate a number of services and therefore must annotate them according to some shared ontology. ASSAM is also relevant for service producers who have deployed a Web Service and want to make it compatible with an existing ontology. ASSAM's capabilities to automatically create semantic metadata are supported by two machine learning algorithms. First, we have developed an iterative relational classification algorithm for semantically classifying Web Services, their operations, and input and output messages. Second, to aggregate the data returned by multiple semantically related Web Services, we have developed a schema mapping algorithm that is based on an ensemble of string distance metrics.},
author = {He\ss, Andreas and Johnston, Eddie and Kushmerick, Nicholas},
doi = {10.1007/978-3-540-30475-3\_23},
file = {:home/fpierin/Dropbox/biblioteca/hess-iswc04-demo.pdf:pdf},
isbn = {978-3-540-23798-3},
issn = {03029743},
journal = {3rd International Semantic Web Conference (ISWC 2004)},
title = {{Assam: A tool for semi-automatically annotating semantic web services}},
url = {http://www.springerlink.com/index/a3g85u9abhjxtbk5.pdf},
year = {2004}
}

@article{Yesilada2008,
abstract = {The World Wide Web (Web) is in transition; a fundamental evolution of the model which underpins the traditional Web. This new Web, Web 2.0, is a mesh of enhanced semantics, push application widgets, and embedded scripting languages and was developed ...},
author = {Yesilada, Yeliz and Harper, Simon},
doi = {10.1145/1340779.1340782},
file = {:home/fpierin/Dropbox/biblioteca/p19-yesilada.pdf:pdf},
issn = {15582337},
journal = {SIGACCESS Access Comput},
number = {90},
pages = {19--31},
title = {{Web 2.0 and the Semantic Web: Hindrance or Opportunity? W4A -- International Cross-Disciplinary Conference on Web Accessibility 2007}},
url = {http://portal.acm.org/citation.cfm?id=1340779.1340782},
year = {2008}
}

@article{Mustapasa2010,
abstract = {Semantic Web is a product of Web 2.0 (second generation of web) that is supported with automated semantic agents for processing user data to help the user on ease of use and personalization of services. Web Mining is an application of data mining which focuses on discovering patterns from Web logs and data. The semantic structure can be built with the pattern or relation results discovered via web mining. By combining those two applications of both disciplines, it's possible to achieve Semantic Web Mining which is a recent hot topic in educational research. This paper gives an overview of current applications of Semantic Web Mining on e-learning which already became a base component of education. ?? 2010 Elsevier Ltd. All rights reserved.},
author = {Mustapaşa, O\v{g}uz and Karahoca, Dilek and Karahoca, Adem and Y{\"{u}}cel, Ahmet and Uzunboylu, Huseyin},
doi = {10.1016/j.sbspro.2010.03.949},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mustapaşa et al. - 2010 - Implementation of Semantic Web Mining on E-Learning.pdf:pdf},
isbn = {1877-0428$\backslash$n*****************},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Semantic web,distance learning,e-learning,personalization,web mining},
number = {2},
pages = {5820--5823},
title = {{Implementation of Semantic Web Mining on E-Learning}},
volume = {2},
year = {2010}
}

@book{Allemang2011,
abstract = {The Semantic Web looks familiar to those accustomed to various sorts of knowledge modeling. The modeling structure that is examined does have a strong connection to a heritage of knowledge modeling languages. However, there is something new that has come along since the early days of expert systems and object-oriented programming; something that has had a far more revolutionizing effect on culture, business, commerce, education, and society than any expert system designer ever dreamed of. It is something so revolutionary that it is often compared in cultural significance to the invention of the printing press. That something new is the World Wide Web. The Semantic Web is the application of advanced technologies that have been used in the context of artificial intelligence, expert systems, and business rules execution in the context of a World Wide Web of information. The Semantic Web is not simply an application running on the Web somewhere; it is a part of the very infrastructure of the Web. It isn't on the Web; it is the Web.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Allemang, Dean and Hendler, Jim},
booktitle = {Semantic Web for the Working Ontologist},
doi = {10.1016/B978-0-12-385965-5.10016-0},
eprint = {arXiv:1011.1669v3},
file = {:home/fpierin/Dropbox/Livros/Ontologist.pdf:pdf},
isbn = {9780123859655},
issn = {9780123859655 0123859654},
pages = {335--338},
pmid = {21931231},
publisher = {Elsevier},
title = {{Semantic Web for the Working Ontologist}},
url = {http://www.sciencedirect.com/science/article/pii/B9780123859655100160},
year = {2011}
}

@article{Yong-gui2010,
abstract = {A semantic-based Web mining is mentioned by many people in order to improve Web service levels and address the existing Web services which is supported by the lack of semantic problem. Semantic-based Web data mining is a combination of the semantic Web and Web mining. Web mining results help to build the semantic Web. The knowledge of Semantic Web makes Web mining easier to achieve, but also can improve the effectiveness of Web mining. This paper firstly introduces the related knowledge of Semantic Web and Web mining, and then discusses the semantic-based Web mining, finally proposes to build a semantic-based Web mining model under the framework of the Agent.},
author = {Yong-gui, Wang and Zhen, Jia},
doi = {10.1109/ICCDA.2010.5541057},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yong-gui, Zhen - 2010 - Research on semantic Web mining.pdf:pdf},
isbn = {9781424471645},
journal = {Computer Design},
keywords = {agent,ontology,semantic web,web ming},
number = {Iccda},
pages = {67--70},
title = {{Research on Semantic Web Mining}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=5541057},
volume = {1},
year = {2010}
}


@article{Handschuh2003,
abstract = {The success of the SemanticWeb crucially depends on the easy creation, integration and use of seman- tic data. For this purpose, we consider an integra- tion scenario that defies core assumptions of cur- rent metadata construction methods. We describe a framework of metadata creation when web pages are generated from a database and the database owner is cooperatively participating in the Seman- tic Web. This leads us to the definition of onto- logy mapping rules by manual semantic annotation and the usage of the mapping rules and of web services for semantic queries. In order to create metadata, the framework combines the presenta- tion layer with the data description layer—in con- trast to “conventional” annotation, which remains at the presentation layer. Therefore, we refer to the framework as deep annotation.1 We consider deep annotation as particularly valid because, (i), web pages generated from databases outnumber static web pages, (ii), annotation of web pages may be a very intuitive way to create se- mantic data from a database and, (iii), data from databases should not be materialized as RDF files, it should remain where it can be handled most effi- ciently—in its databases.},
author = {Handschuh, Siegfried and Staab, Steffen and Volz, Raphael},
doi = {10.1145/775213.775214},
file = {:home/fpierin/Dropbox/biblioteca/www12-deep-annotation.pdf:pdf},
isbn = {1581136803},
journal = {Proceedings of the twelfth international conference on World Wide Web - WWW '03},
pages = {431},
title = {{On deep annotation}},
url = {http://portal.acm.org/citation.cfm?doid=775152.775214$\backslash$nhttp://www.isi.edu/info-agents/workshops/ijcai03/papers/Handschuh.pdf},
year = {2003}
}

@article{Craven2000,
author = {Craven, Mark and Freitag, Dayne},
doi = {10.1016/S0004-3702(00)00004-7},
file = {:home/fpierin/Dropbox/biblioteca/webkb-aij00.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {November 1999},
pages = {69--113},
title = {{Learning to Construct Knowledge Bases from}},
year = {2000}
}

@article{Hess2003,
abstract = {Emerging Web standards promise a network of heterogeneous yet interoperable Web Services. Web Services would greatly simplify the development of many kinds of data integration and knowledge management applications. Unfortunately, this vision requires that services describe themselves with large amounts of semantic metadata ``glue''. We explore a variety of machine learning techniques to semi-automatically create such metadata.},
author = {He\ss, Andreas and Kushmerick, Nicholas},
doi = {10.1007/978-3-540-39718-2\_17},
file = {:home/fpierin/Dropbox/biblioteca/hess-iswc03.pdf:pdf},
issn = {03029743},
journal = {The Semantic Web - ISWC 2003},
pages = {258--273},
title = {{Learning to Attach Semantic Metadata to Web Services}},
volume = {2870},
year = {2003}
}

@misc{PortalDadosAbertos,
abstract = {O Portal Brasileiro de Dados Abertos \'{e} a ferramenta disponibilizada pelo governo para que todos possam encontrar e utilizar os dados e as informa\c{c}\~{o}es p\'{u}blicas. O portal preza pela simplicidade e organiza\c{c}\~{a}o para que voc\^{e} possa encontrar facilmente os dados e informa\c{c}\~{o}es que precisa. O portal tamb\'{e}m tem o objetivo de promover a interlocu\c{c}\~{a}o entre atores da sociedade e com o governo para pensar a melhor utiliza\c{c}\~{a}o dos dados em prol de uma sociedade melhor.},
title = {{Portal brasileiro de dados abertos}},
url = {http://dados.gov.br/sobre/},
urldate = {05/02/2015}
}

@inproceedings{Berners-lee2001,
abstract = {A modern business process management (BPM) operates using common tenants of an underlying Service Oriented Architecture (SOA) runtime infrastructure based on the Service Component Architecture (SCA) and supports the BPMN 2.0 OMG $\backslash$n 1 standard. Semantically-enabling all BPM artifacts, from high-level design to deployment and the runtime model of a BPM application, promotes continuous process refinement, comprehensive impact analysis, and reuse to minimize process and service proliferation. A semantic database can manage semantically-enabled BPM ontologies and models, enable machine-driven inference to discover implicit relationships in the models, and perform pattern-matching queries to find associations. This paper presents an ontology for BPM based upon BPMN 2.0, Service Component Architecture (SCA) and the Web Ontology Language (OWL 2) that can support a wide range of use cases for process analysis, governance, business intelligence and systems management. It has the potential to bring together stakeholders across an enterprise, for a truly agile, end-to-end enterprise architecture. {\textcopyright} 2012 Springer-Verlag.},
archivePrefix = {arXiv},
arxivId = {1204.6441},
author = {Berners-Lee, T and Hendler, J and Lassila, O},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-89704-0},
eprint = {1204.6441},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berners-lee, Hendler - 2001 - The Semantic Web.pdf:pdf},
isbn = {978-3-540-89703-3},
issn = {0036-8733},
keywords = {BPM Ontology,BPMN 2.0,OWL 2,SCA,SOA,SPARQL,Semantic Technologies},
pages = {402--410},
pmid = {122},
title = {{The Semantic Web}},
url = {http://www.scientificamerican.com/article.cfm?id=the-semantic-web},
volume = {5926},
year = {2002}
}

@article{Alani2007,
author = {Alani, Harith and Dupplaw, David and Sheridan, John and O'Hara, K},
file = {:home/fpierin/Dropbox/biblioteca/chp\%3A10.1007\%2F978-3-540-76298-0\_51.pdf:pdf},
keywords = {and carol tullo 2,david dupplaw 1,hara 1,harith alani 1,john darlington 1,john sheridan 2,kieron o,nigel shadbolt 1,ocking the potential of,public sector information,with semantic web technology},
pages = {708--721},
title = {{Unlocking the potential of public sector information with semantic web technology}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-76298-0\_51},
year = {2007}
}

@article{Klischewski2006,
abstract = {As the e-government domain is about to become a field of application for Semantic Web technologies, the actors involved still lack reasoning to decide on critical issues such as organisational cost/benefit, user involvement, technical integration, and implementation strategy. Firstly, the paper seeks to identify semantic problems in e-government as prerequisite for discussing the requirements for the application of Semantic Web technologies. Secondly, experiences from an ongoing project are discussed to identify critical issues from the systems development perspective. Thirdly, taking into account the problems identified and the case findings, a research agenda is laid out aiming to guide and support the application of Semantic Web technologies in e-government.},
author = {Klischewski, Ralf},
doi = {10.1007/10929179_52},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klischewski - Unknown - Semantic Web for e-Government.pdf:pdf},
isbn = {03029743 (ISSN)},
issn = {03029743},
journal = {Electronic Government},
keywords = {e-government},
number = {June},
pages = {288--295},
title = {{Semantic Web for e-Government}},
url = {http://www.springerlink.com/openurl.asp?genre=article{\&}id=69RMTUDPR46Q5B9N},
volume = {2739},
year = {2006}
}

@article{Janssen2012,
abstract = {In this article, based on data collected through interviews and a workshop, the benefits and adoption barriers for open data have been derived. The results suggest that a conceptually simplistic view is often adopted with regard to open data, which automatically correlates the publicizing of data with use and benefits. Also, five "myths" concerning open data are presented, which place the expectations within a realistic perspective. Further, the recommendation is provided that such projects should take a user's view.},
author = {Janssen, Marijn and Charalabidis, Yannis and Zuiderwijk, Anneke},
doi = {10.1080/10580530.2012.716740},
file = {:home/fpierin/Dropbox/biblioteca/Author version Myths and benefits of Open data and Government ISM2012.pdf:pdf},
isbn = {1058-0530},
issn = {1058-0530},
journal = {Information Systems Management},
month = sep,
number = {4},
pages = {258--268},
title = {{Benefits, Adoption Barriers and Myths of Open Data and Open Government}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10580530.2012.716740},
volume = {29},
year = {2012}
}

@misc{Kushmerick1997,
abstract = {Many Internet information resources present relational data--telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is difficult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify HLRT, a wrapper class that is efficiently learnable, yet expressive enough to handle 48{\%} of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem's sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge.},
author = {Kushmerick, Nicholas and Weld, Daniel S and Doorenbos, R},
booktitle = {Intl Joint Conference on Artificial Intelligence IJCAI},
doi = {10.1.1.33.2176},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Wrapper induction for information extraction.pdf:pdf},
isbn = {0591708434},
pages = {729--737},
title = {{Wrapper induction for information extraction}},
url = {citeseer.ist.psu.edu/kushmerick97wrapper.html},
year = {1997}
}

@article{Singh2010,
abstract = {Web Data Mining is an important area of Data Mining which deals with the extraction of interesting knowledge from the World Wide Web, It can be classified into three different types i.e. web content mining, web structure mining and web usages mining. The aim of this paper is to provide past, current evaluation and update in each of the three different types of web mining i.e. web content mining, web structure mining and web usages mining and also outlines key future research directions. This paper also reports the comparisons and summary of various methods of web data mining with applications, which gives the overview of development in research and some important research issues.},
annote = {Fala sobre maneiras de extrair os dados de documentos de texto. Eh relevante a pesquisa no sentido em que fala mais sobre os estudos sobre web content mining.},
archivePrefix = {arXiv},
arxivId = {cs/0011033v1},
author = {Singh, Brijendra and Singh, Hemant Kumar},
doi = {10.1109/ICCIC.2010.5705856},
eprint = {0011033v1},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Singh - Unknown - WEB DATA MINING RESEARCH A SURVEY.pdf:pdf},
isbn = {978-1-4244-5965-0},
issn = {19310145},
journal = {IEEE International Conference on Computational Intelligence and Computing Research},
keywords = {Web mining,semantic web,web content mining,web structure mining,web usage mining},
pages = {1--10},
primaryClass = {cs},
title = {{Web Data Mining research: A survey}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5705856},
year = {2010}
}

@article{Nardin2011,
abstract = {In the last decades, we have experienced a rapid increase in the number of available online e-services. Agent-based computing has been advocated as a natural computational model to automate the interaction with those services, thus enabling the formation of multiagent systems. In these latter, agents may use trust and reputation as the main control mechanism and they usually exchange such information in order to accelerate reputation evaluation. However, due to the semantic heterogeneity of the different reputation models, agents interaction about reputation has to deal with interoperability issues. Therefore, this paper presents some experiments using SOARI, an architecture that enables the semantic interoperability among agents that have heterogeneous reputation models. Such experiments were conducted using two reputation testbeds and three agent reputation models in order to analyze the accuracy of the agents reputation evaluation in the presence of a more expressive communication apparatus, as well as the effect of the heterogeneity among reputation models on this accuracy. © 2011 Elsevier Ltd. All rights reserved.},
author = {Nardin, Luis G. and Brand{\~{a}}o, Anarosa a F and Sichman, Jaime S.},
doi = {10.1016/j.engappai.2011.05.004},
file = {:home/fpierin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nardin, Brand{\~{a}}o, Sichman - 2011 - Experiments on semantic interoperability of agent reputation models using the SOARI architecture.pdf:pdf},
isbn = {0952-1976},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Artificial intelligence,Multiagent systems,Reputation,Semantic interoperability,Service oriented architecture},
number = {8},
pages = {1461--1471},
title = {{Experiments on semantic interoperability of agent reputation models using the SOARI architecture}},
volume = {24},
year = {2011}
}

@article{Bizer2009,
abstract = {The term Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions - the Web of Data. In this article we present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. We describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Bizer, Christian and Heath, T and Berners-Lee, T},
doi = {10.4018/jswis.2009081901},
eprint = {1011.1669},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/bizer-heath-berners-lee-ijswis-linked-data.pdf:pdf},
isbn = {1552-6283},
issn = {15526283},
journal = {International journal on Semantic Web and Information Systems},
keywords = {data exploration,data sharing,linked data,semantic web,web of data},
number = {3},
pages = {1--22},
pmid = {16},
title = {{Linked data-the story so far}},
url = {http://eprints.soton.ac.uk/271285/},
volume = {5},
year = {2009}
}

@article{Brickley2010,
abstract = {This specification describes the FOAF language, defined as a dictionary of named properties and classes using W3C's RDF technology.},
author = {Brickley, Dan and Miller, Libby},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/s14.FOAF Vocabulary Specification.pdf:pdf},
journal = {Namespace Document},
number = {August},
pages = {http://xmlns.com/foaf/spec/},
title = {{FOAF Vocabulary Specification}},
url = {http://xmlns.com/foaf/spec/},
volume = {3},
year = {2010}
}

@article{bizer2009linked,
  title={Linked data-the story so far},
  author={Bizer, Christian and Heath, Tom and Berners-Lee, Tim},
  journal={Semantic services, interoperability and web applications: emerging concepts},
  pages={205--227},
  year={2009}
}

@article{Peffers2007,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
archivePrefix = {arXiv},
arxivId = {z0022},
author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus A. and Chatterjee, Samir},
doi = {10.2753/MIS0742-1222240302},
eprint = {z0022},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/A Design Science Research Methodology for Information Systems Research.pdf:pdf},
isbn = {0742-1222},
issn = {0742-1222},
journal = {Journal of Management Information Systems},
number = {3},
pages = {45--77},
pmid = {28843849},
title = {{A Design Science Research Methodology for Information Systems Research}},
url = {http://www.tandfonline.com/doi/full/10.2753/MIS0742-1222240302},
volume = {24},
year = {2007}
}

@article{Gregor2013,
abstract = {Design science research (DSR) has staked its rightful ground as an important and legitimate Information Systems (IS) research paradigm. We contend that DSR has yet to attain its full potential impact on the devel- opment and use of information systems due to gaps in the understanding and application of DSR concepts and methods. This essay aims to help researchers (1) appreciate the levels of artifact abstractions that may be DSR contributions, (2) identify appropriate ways of consuming and producing knowledge when they are preparing journal articles or other scholarly works, (3) understand and position the knowledge contributions of their research projects, and (4) structure a DSR article so that it emphasizes significant contributions to the knowl- edge base. Our focal contribution is the DSR knowledge contribution framework with two dimensions based on the existing state of knowledge in both the problem and solution domains for the research opportunity under study. In addition, we propose a DSR communication schema with similarities to more conventional publica- tion patterns, but which substitutes the description of the DSR artifact in place of a traditional results section. We evaluate the DSR contribution framework and the DSR communication schema via examinations of DSR exemplar publications},
archivePrefix = {arXiv},
arxivId = {z0022},
author = {Gregor, Shirley and Hevner, Alan R},
doi = {10.2753/MIS0742-1222240302},
eprint = {z0022},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/gregor-2013-positioning-presenting-design-science-research.pdf:pdf},
isbn = {02767783},
issn = {02767783},
journal = {MIS Quarterly},
keywords = {DSR theory,Design science research (DSR),computer science discipline,design artifact,engineering discipline,information systems,knowledge,knowledge contribution framework,publication schema},
number = {2},
pages = {337--355},
pmid = {28843849},
title = {{P OSITIONING AND P RESENTING D ESIGN S CIENCE Types of Knowledge in Design Science Research}},
volume = {37},
year = {2013}
}

@article{Peffers2006,
abstract = {The authors design and demonstrate a process for carrying out design science (DS) research in information systems and demonstrate use of the process to conduct re- search in two case studies. Several IS researchers have pioneered the acceptance of DS research in IS, but in the last 15 years little DS research has been done within the discipline. The lack of a generally accepted process for DS research in IS may have contributed to this problem. We sought to design a design science research process (DSRP) model that would meet three objectives: it would be consistent with prior lit- erature, it would provide a nominal process model for doing DS research, and it would provide a mental model for presenting and appreciating DS research in IS. The process includes six steps: problem identification and motivation, objectives for a solution, design and development, evaluation, and communication. We demonstrated the process by using it in this study and by presenting two case studies, one in IS planning to develop application ideas for mobile financial services and another in re- quirements engineering to specify feature requirements for a self service advertising design and sales system intended for wide audience end users. The process effectively satisfies the three objectives and has the potential to help aid the acceptance of DS research in the IS discipline. Keywords:},
archivePrefix = {arXiv},
arxivId = {z0022},
author = {Peffers, Ken and Tuunanen, Tuure and Gengler, Charles E and Rossi, Matti and Hui, Wendy and Virtanen, Ville and Bragge, Johanna},
doi = {10.2753/MIS0742-1222240302},
eprint = {z0022},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/000designscresearchproc{\_}desrist{\_}2006.pdf:pdf},
isbn = {1702807118},
issn = {0742-1222},
journal = {Proceedings of Design Research in Information Systems and Technology DESRIST'06},
keywords = {Design science,and information systems development.,case study,design science research process,process model,requirements elicitation,requirements engineering},
pages = {83--106},
pmid = {28843849},
title = {{The Design Science Research Process: A Model for Producing and Presenting Information Systems Research}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+Design+Science+Research+Process:+A+Model+for+Producing+and+presenting+Information+Systems+Research{\#}0},
volume = {24},
year = {2006}
}

@article{Hevner2004,
abstract = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioralscience paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hevner and March and Park and Ram},
doi = {10.2307/25148625},
eprint = {arXiv:1011.1669v3},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/25148625.pdf:pdf},
isbn = {02767783},
issn = {02767783},
journal = {MIS Quarterly},
number = {1},
pages = {75},
pmid = {6938092},
title = {{Design Science in Information Systems Research}},
url = {http://www.jstor.org/stable/10.2307/25148625},
volume = {28},
year = {2004}
}

@inproceedings{Wang2017,
abstract = {Software-defined networking (SDN) provides an unprecedented op-portunity to exercise computing principles in networking practice. This paper investigates data integration, a under-explored disci-pline from the database community. We propose to manage the various SDN control applications that collectively drive a shared dataplane using a data integration system. First, we develop a base-line design and study its feasibility on two networking challenges not adequately addressed in classic data integration systems: the extensibility requirement to add new controls on demand, and the performance requirement to cope with fast dataplane updates. Cen-tral to our baseline design is a relational model, where the entire SDN is represented as relational data (values in tables) with distinct roles. The control applications act as data sources generating net-work state, and the dataplane becomes the integrated whole. Based on this model, we explore extensions to data integration systems called behavioral dependency, a formal notion that captures the dy-namic interactions between the control applications. While our de-sign and extension are not intended to provide a comprehensive so-lution, we believe our study is a step toward reaping the benefits of data integration for SDN.},
address = {New York, New York, USA},
author = {Wang, Anduo and Croft, Jason and Dragut, Eduard},
booktitle = {Proceedings of the ACM International Workshop on Security in Software Defined Networks {\&} Network Function Virtualization - SDN-NFVSec '17},
doi = {10.1145/3040992.3041006},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/reflection{\_}short.pdf:pdf},
isbn = {9781450349086},
pages = {65--68},
publisher = {ACM Press},
title = {{Reflections on Data Integration for SDN}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018293907{\&}doi=10.1145{\%}2F3040992.3041006{\&}partnerID=40{\&}md5=16dcd67b46c6e6a0b6f69717fd715ec2 http://dl.acm.org/citation.cfm?doid=3040992.3041006},
year = {2017}
}


@inproceedings{Putra2017,
author = {Putra, Syopiansyah Jaya and Khalil, Ismail},
booktitle = {2017 5th International Conference on Cyber and IT Service Management (CITSM)},
doi = {10.1109/CITSM.2017.8089317},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/08089317.pdf:pdf},
isbn = {978-1-5386-2739-6},
keywords = {context,contextual search,intelligence search,search},
month = {aug},
pages = {1--4},
publisher = {IEEE},
title = {{Context for the intelligent search of information}},
url = {http://ieeexplore.ieee.org/document/8089317/},
year = {2017}
}


@inproceedings{Abdellaoui2015,
abstract = {Competitive Intelligence (CI) is the process of managing information
emanating from the business environment of an organization in order to
support decision making process. CI enables the development of
strategies that confer companies a significant competitive advantage.
The appropriate decisions are taken when all required data are
considered. As the amount of data grows very fast inside and outside of
an enterprise, exploiting these mountains of data efficiently became a
crucial need. Note that such data are usually multiples, heterogeneous,
autonomous and distributed. The main issue is related to the
identification and resolution of structural and semantic heterogeneity
between data, usually spread in multiple sources. Data integration is a
significant solution that addresses these problems. Data Warehouse (DW)
is viewed as Data Integration System (DIS) that consolidates several
data sources in the same target repository through an
Extract-Transform-Load (ETL) process. In a decisional context, DW is a
relevant solution to aggregate a huge amount of data and organizing them
in order to facilitate their analysis and support decision making. In
this paper, we propose an approach for designing Semantic Data Warehouse
(SDW) to support CI process. Our proposal takes as inputs decision
makers requirements and a set of Semantic Databases (SDB) sources for
building the SDW.},
author = {Abdellaoui, Sabrina and Nader, Fahima},
booktitle = {2015 6Th International Conference on Information Systems and Economic Intelligence (Siie)},
doi = {10.1109/ISEI.2015.7358736},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/07358736.pdf:pdf},
isbn = {978-1-4799-8934-8},
keywords = {Competitive intelligence; Integration Systems; Sem},
month = {feb},
pages = {141--145},
publisher = {IEEE},
title = {{Semantic Data Warehouse at the heart of Competitive Intelligence Systems: design approach}},
url = {http://ieeexplore.ieee.org/document/7358736/},
year = {2015}
}

@article{Dogac2004,
abstract = {Today, the travel information services are dominantly provided by Global Distribution Systems(GDS). The Global Distribution Systems provide access to real time availability and price information for flights, hotels and car rental companies. However GDSs have legacy architectures with private networks, specialized hardware, limited speed and search capabilities. Furthermore, being legacy systems, it is very difficult to interoperate them with other systems and data sources. For these reasons, Web service technology is an ideal fit for travel information systems. However to be able to exploit Web services to their full potential, it is necessary to introduce semantics. Without describing the semantics of Web services we are looking for, it is difficult to find them in an automated way and if we cannot describe the service we have, the probability that people will find it in an automated way is low. Furthermore, to make the semantics machine process able and interoperable, we need to describe domain knowledge through standard ontology languages. In this paper, we describe how to deploy semantically enriched travel Web services and how to exploit semantics through Web service registries. We also address the need to use semantics in discovering both Web services and Web service registries through peer-to-peer technology.},
author = {Dogac, a. and Kabak, Y. and Laleci, G. and Sinir, S. and Yildiz, A. and Kirbas, S. and Gurcan, Y.},
doi = {10.1145/1031570.1031575},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/p21-dogac.pdf:pdf},
isbn = {1581134975},
issn = {01635808},
journal = {ACM SIGMOD Record},
month = {sep},
number = {3},
pages = {21},
title = {{Semantically enriched web services for the travel industry}},
url = {http://portal.acm.org/citation.cfm?doid=1031570.1031575},
volume = {33},
year = {2004}
}

@article{Balduini2012,
abstract = {In 2011, an average of three million tweets per day was posted in Seoul. Hundreds of thousands of tweets carry the live opinion of some tens of thousands of users about restaurants, bars, cafes, and many other semi-public points of interest (POIs) in the city. Trusting this collective opinion to be a solid base for novel commercial and social services, we conceived BOTTARI: an augmented reality application that offers personalized and localized recommendation of POIs based on the temporally weighted opinions of the social media community. In this paper, we present the design of BOTTARI, the potentialities of semantic technologies such as inductive and deductive stream reasoning, and the lessons learnt in experimentally deploying BOTTARI in Insadong-a popular tourist area in Seoul-for which we have been collecting tweets for three years to rate the hundreds of restaurants in the district. The results of our study demonstrate the feasibility of BOTTARI and encourage its commercial spread. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Balduini, Marco and Celino, Irene and Dell'Aglio, Daniele and {Della Valle}, Emanuele and Huang, Yi and Lee, Tony and Kim, Seon Ho and Tresp, Volker},
doi = {10.1016/j.websem.2012.06.004},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/1-s2.0-S157082681200073X-main.pdf:pdf},
isbn = {1570-8268},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Location-based recommendation,Mobile app,Personalized recommendation,Social media analysis,Stream reasoning},
month = {nov},
pages = {33--41},
publisher = {Elsevier B.V.},
title = {{BOTTARI: An augmented reality mobile application to deliver personalized and location-based recommendations by continuous analysis of social media streams}},
url = {http://dx.doi.org/10.1016/j.websem.2012.06.004 http://linkinghub.elsevier.com/retrieve/pii/S157082681200073X},
volume = {16},
year = {2012}
}


@book{Cardoso2006,
abstract = {Semantics, Web services, and Web processes promise better re-use, universal interoperability and integration. Semantics has been recognized as the primary tool to address the challenges of a broad spectrum of heterogeneity and for improving automation through machine understandable descriptions. Semantic Web Services, Processes and Applications brings contributions from researchers who study, explore and understand the semantic enabling of all phases of semantic Web processes. This encompasses design, annotation, discovery, choreography and composition. Also this book presents fundamental capabilities and techniques associated with ontological modeling or services, annotation, matching and mapping, and reasoning. This is complemented by discussion of applications in e-Government and bioinformatics. Special bulk rates are available for course adoption through Publishing Editor.},
address = {Boston, MA},
doi = {10.1007/978-0-387-34685-4},
editor = {Cardoso, Jorge and Sheth, Amit P.},
file = {:home/fpierin/Dropbox/USP/Mestrado/Disserta{\c{c}}{\~{a}}o/artigos/Developing{\_}An{\_}Owl{\_}Ontology{\_}For{\_}e-Tourism.pdf:pdf},
isbn = {978-0-387-30239-3},
month = {May},
pages = {0--33},
publisher = {Springer US},
series = {Semantic Web and Beyond},
title = {{Semantic Web Services, Processes and Applications}},
url = {http://link.springer.com/10.1007/978-0-387-34685-4},
volume = {3},
year = {2006}
}

@INPROCEEDINGS{RodriguezMuro2013OntopAW,
abstract = {We describe the architecture of the OBDA system Ontop and analyse its performance in a series of experiments. We demonstrate that, for standard ontologies, queries and data stored in relational databases, Ontop is fast, efficient and produces SQL rewritings of high quality.},
author = {Mariano Rodríguez-Muro and Roman Kontchakov and Michael Zakharyaschev},
title = {Ontop at work},
booktitle = {PROC. OF OWL: EXPERIENCES AND DIRECTIONS WORKSHOP 2013 (OWLED 2013)},
year = {2013},
publisher = {CEUR-WS}
}